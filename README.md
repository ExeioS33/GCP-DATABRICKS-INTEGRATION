# Projet G√©n√©ration Donn√©es Synth√©tiques GCP/Databricks

Pipeline de g√©n√©ration de donn√©es financi√®res synth√©tiques optimis√© pour GCP et Databricks, r√©alis√© par Sanda ANDRIA et Celia HADJI.

**Date de derni√®re mise √† jour :** 05 Juin 2025

## üìã **Vue d'ensemble**

Ce projet g√©n√®re 10 millions de transactions financi√®res synth√©tiques (~2GB) avec 19 types d'op√©rations r√©alistes pour l'analyse dans Databricks. Il propose une solution multi-thread locale ultra-performante, fruit d'une optimisation intensive lors de notre premi√®re journ√©e de travail.

**Performance Atteinte :** 10M lignes g√©n√©r√©es et upload√©es sur GCS en **3-5 minutes**.

## üèóÔ∏è **Structure du Projet**

```
‚îú‚îÄ‚îÄ üìÅ docs/                           # Documentation compl√®te
‚îÇ   ‚îú‚îÄ‚îÄ README.md                       # Ce guide d√©taill√©
‚îÇ   ‚îú‚îÄ‚îÄ RAPPORT_TECHNIQUE_GENERATION_DONNEES.md  # Rapport technique complet (par Sanda & Celia)
‚îÇ   ‚îú‚îÄ‚îÄ RAPPORT_DEBOGAGE.md            # Historique debugging (ancienne approche Dataflow)
‚îÇ   ‚îî‚îÄ‚îÄ GCP_GCLOUD_CHEATSHEET.md       # Commandes GCP utiles
‚îÇ
‚îú‚îÄ‚îÄ üìÅ src/                            # Code source principal
‚îÇ   ‚îî‚îÄ‚îÄ generate_csv_fast_fixed.py     # G√©n√©rateur multi-thread optimis√© ‚≠ê
‚îÇ
‚îú‚îÄ‚îÄ üìÅ scripts/                        # Scripts d'ex√©cution et tests
‚îÇ   ‚îú‚îÄ‚îÄ run_fast_generation.py         # Lancement g√©n√©ration rapide ‚≠ê
‚îÇ   ‚îú‚îÄ‚îÄ test_fast_generation.py        # Tests validation
‚îÇ   ‚îú‚îÄ‚îÄ deploy.sh                      # D√©ploiement infrastructure (Terraform)
‚îÇ   ‚îî‚îÄ‚îÄ create_gcs_structure.sh        # Ancien script (maintenu pour r√©f√©rence)
‚îÇ
‚îú‚îÄ‚îÄ üìÅ infrastructure/                 # Infrastructure as Code
‚îÇ   ‚îî‚îÄ‚îÄ terraform/                     # Configuration Terraform
‚îÇ       ‚îú‚îÄ‚îÄ main.tf                    # Ressources GCP compl√®tes
‚îÇ       ‚îî‚îÄ‚îÄ terraform.tfvars.example   # Variables d'exemple
‚îÇ
‚îú‚îÄ‚îÄ üìÅ notebooks/                      # Notebooks Databricks pour l'analyse
‚îÇ   ‚îî‚îÄ‚îÄ 01_ingestion_brute_gcs.ipynb   # Ingestion GCS -> Databricks
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt                   # D√©pendances Python (pour uv)
‚îú‚îÄ‚îÄ pyproject.toml                     # Configuration uv/Python
‚îî‚îÄ‚îÄ .gitignore                         # Fichiers ignor√©s par Git
```

## üöÄ **D√©marrage Rapide**

### **1. G√©n√©ration Rapide des Donn√©es (Recommand√©e)**

```bash
# Assurez-vous que l'environnement virtuel est activ√©
# (source .venv/bin/activate ou uv venv)

# G√©n√©ration ultra-rapide de 10M lignes en 3-5 minutes
uv run scripts/run_fast_generation.py
```
Les fichiers CSV seront g√©n√©r√©s localement dans `csv_output/` puis upload√©s sur `gs://supdevinci_bucket/sanda_celia/raw/`.

### **2. D√©ploiement de l'Infrastructure (Si n√©cessaire)**

Si vous partez de z√©ro, d√©ployez l'infrastructure GCS et IAM avec Terraform :
```bash
# Authentification GCP (une seule fois)
gcloud auth login
gcloud auth application-default login

# D√©ploiement automatis√©
./scripts/deploy.sh
```

### **3. Int√©gration Databricks**

Ouvrez le notebook `notebooks/01_ingestion_brute_gcs.ipynb` dans votre workspace Databricks et ex√©cutez les cellules pour :
1. Configurer l'acc√®s s√©curis√© √† GCS.
2. Lire les donn√©es CSV depuis `gs://supdevinci_bucket/sanda_celia/raw/`.
3. Explorer et valider les donn√©es.

## ‚öôÔ∏è **Configuration Pr√©alable**

- **Python 3.12+** avec [uv](https://docs.astral.sh/uv/) (gestionnaire de packages).
- **Google Cloud SDK** (`gcloud`, `gsutil`) configur√©.
- **Credentials GCP** avec permissions sur le projet `biforaplatform` et le bucket `gs://supdevinci_bucket/sanda_celia/`.
- **Terraform** install√© si vous utilisez le script `deploy.sh`.

**Installation des d√©pendances Python :**
```bash
# Cr√©er l'environnement (si pas fait)
uv venv --python 3.12
source .venv/bin/activate # ou √©quivalent

# Synchroniser les d√©pendances
uv sync
```

## üéØ **Donn√©es G√©n√©r√©es**

- **Format :** CSV avec 19 types d'op√©rations financi√®res.
- **Volume :** 10 millions de lignes, environ 2GB.
- **Structure :** `id_mouvement,date_op,produit,type_op,montant,agence_id,pays`.

## üìñ **Documentation D√©taill√©e**

- **[Rapport Technique (Sanda & Celia)](docs/RAPPORT_TECHNIQUE_GENERATION_DONNEES.md)** : Revivez notre premi√®re journ√©e de d√©veloppement et comprenez nos choix d'optimisation.
- **[Ancien Rapport de Debugging Dataflow](docs/RAPPORT_DEBOGAGE.md)** : Pour m√©moire, les d√©fis de l'approche Dataflow initiale.
- **[Aide-M√©moire Commandes GCP](docs/GCP_GCLOUD_CHEATSHEET.md)**.

## üîó **Int√©gration Databricks - Points Cl√©s**

Le notebook `01_ingestion_brute_gcs.ipynb` utilise `dbutils.fs.ls` et `spark.read.csv` pour acc√©der aux donn√©es sur GCS. Assurez-vous que votre cluster Databricks a les permissions n√©cessaires (via un profil d'instance ou une configuration de service account) pour lire depuis le bucket `gs://supdevinci_bucket`.

Exemple de lecture dans Databricks (apr√®s configuration) :
```python
DATA_PATH = "gs://supdevinci_bucket/sanda_celia"
df_raw = spark.read \
    .option("header", "true") \
    .option("inferSchema", "false") \
    .csv(f"{DATA_PATH}/raw/*.csv")
df_raw.show()
```

## üí° **Pourquoi cette solution locale plut√¥t que Dataflow ?**

Notre rapport technique d√©taille notre parcours, mais en r√©sum√© :
- **Performance Brute :** 3-5 minutes contre 30+ minutes.
- **Simplicit√© :** Moins de d√©pendances complexes et de configuration cloud.
- **Fiabilit√© :** √âlimination des erreurs li√©es aux ressources et politiques GCP sp√©cifiques √† Dataflow.
- **Co√ªt :** Pas de co√ªt de compute pour la g√©n√©ration (uniquement stockage GCS).

Pour notre cas d'usage (g√©n√©ration massive de CSV), l'approche locale s'est av√©r√©e largement sup√©rieure.

---

*Projet men√© par Sanda ANDRIA & Celia HADJI - Optimis√© pour uv ‚ö°*