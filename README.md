# üöÄ G√©n√©rateur de Donn√©es Synth√©tiques GCP/Databricks

Ce projet g√©n√®re des donn√©es synth√©tiques de mouvements financiers dans Google Cloud Storage (GCS) en utilisant Apache Beam/Dataflow, optimis√© pour une int√©gration avec Databricks.

## üìä Architecture du Data Lake

```
gs://supdevinci_bucket/sanda_celia/
‚îú‚îÄ‚îÄ tmp/           # Fichiers temporaires Dataflow
‚îú‚îÄ‚îÄ raw/           # Donn√©es brutes (CSV) - Sortie de ce script
‚îú‚îÄ‚îÄ staging/       # Donn√©es optimis√©es (Parquet) - Via Databricks
‚îî‚îÄ‚îÄ delta/         # Tables Delta - Via Databricks
```

## üõ†Ô∏è Installation

### 1. Pr√©requis
- Python 3.12+
- [uv](https://docs.astral.sh/uv/) - Gestionnaire de packages et environnements Python ultra-rapide
- Compte GCP avec Dataflow API activ√©e
- Bucket GCS cr√©√© : `gs://supdevinci_bucket/sanda_celia/`
- gcloud CLI configur√©

### 2. Installation d'uv (si pas d√©j√† fait)
```bash
# Installation d'uv
curl -LsSf https://astral.sh/uv/install.sh | sh

# Ou via pip si vous pr√©f√©rez
pip install uv
```

### 3. Configuration de l'environnement Python
```bash
# Cr√©er et activer l'environnement virtuel avec uv
uv venv --python 3.12

# Activer l'environnement (Linux/macOS)
source .venv/bin/activate

# Activer l'environnement (Windows)
# .venv\Scripts\activate

# Installer les d√©pendances avec uv
uv sync
```

### 4. Configuration GCP
```bash
# Authentification
gcloud auth application-default login

# Configuration du projet (remplacez par votre PROJECT_ID)
gcloud config set project YOUR_PROJECT_ID
```

## ‚öôÔ∏è Configuration

Modifiez le fichier `src/config.py` :

```python
class Config:
    PROJECT_ID = "biforaplatform"  # ‚ö†Ô∏è Modifiez avec votre PROJECT_ID
    REGION = "europe-west1"        # R√©gion de votre choix
    # ... autres param√®tres
```

Ou utilisez les variables d'environnement :
```bash
export GCP_PROJECT_ID="biforaplatform"
export GCP_REGION="europe-west1"
export GCS_BUCKET_BASE="gs://supdevinci_bucket/sanda_celia"
```

## üöÄ Utilisation

### G√©n√©ration des donn√©es
```bash
# Avec uv (recommand√©)
uv run src/generate_to_gcs.py

# Ou si l'environnement est activ√©
python src/generate_to_gcs.py
```

### Afficher le guide Databricks
```bash
uv run src/generate_to_gcs.py --guide
```

### V√©rification des donn√©es g√©n√©r√©es
```bash
# V√©rification basique
uv run src/verify_data.py

# Avec √©chantillon des donn√©es
uv run src/verify_data.py --sample

# Avec statistiques d√©taill√©es
uv run src/verify_data.py --stats
```

## üìà Structure des Donn√©es G√©n√©r√©es

### Schema CSV
| Champ          | Type   | Description                     | Exemple                         |
| -------------- | ------ | ------------------------------- | ------------------------------- |
| `id_mouvement` | string | Identifiant unique du mouvement | M0000001                        |
| `date_op`      | date   | Date de l'op√©ration             | 2025-06-15                      |
| `produit`      | string | Type de produit d'assurance     | auto, sant√©, habitation, vie    |
| `type_op`      | string | Type d'op√©ration financi√®re     | cotisation, remboursement, etc. |
| `montant`      | float  | Montant de l'op√©ration (‚Ç¨)      | 1250.50                         |
| `agence_id`    | string | Identifiant de l'agence         | AG_001                          |
| `pays`         | string | Code pays                       | FR, ES, IT, DE, BE              |

### Types d'op√©rations financi√®res (19 types)
- **Op√©rations de base** : cotisation, remboursement, commission
- **Ajustements** : r√©trocession, ajustement, r√©gularisation
- **P√©nalit√©s** : p√©nalit√©, correction, annulation
- **Op√©rations internes** : virement, provision, r√©assurance
- **Autres** : frais_gestion, prime_exceptionnelle, bonus_malus, etc.

## üèóÔ∏è Architecture Technique

### Pipeline de donn√©es
1. **G√©n√©ration** (Apache Beam/Dataflow) ‚Üí GCS RAW (CSV)
2. **Transformation** (Databricks) ‚Üí GCS STAGING (Parquet)
3. **Optimisation** (Databricks) ‚Üí GCS DELTA (Tables Delta)

### Volumes et performances
- **Volume cible** : ~10M lignes/mois (~2 Go)
- **Fichiers de sortie** : 5 fichiers de ~400MB chacun
- **Format initial** : CSV avec ent√™tes
- **Partitioning Databricks** : Par pays et produit

## üîß Commandes utiles avec uv

### Gestion de l'environnement
```bash
# Cr√©er un nouvel environnement
uv venv --python 3.12

# Installer une nouvelle d√©pendance
uv add apache-beam[gcp]

# Installer en mode d√©veloppement
uv add --dev pytest

# Synchroniser les d√©pendances
uv sync

# Ex√©cuter un script
uv run src/generate_to_gcs.py

# Ouvrir un shell Python dans l'environnement
uv run python
```

### Tests et d√©veloppement
```bash
# Installer les d√©pendances de test
uv add --dev pytest pytest-cov

# Ex√©cuter les tests
uv run pytest

# Linter le code
uv add --dev ruff
uv run ruff check src/
```

## üìÅ Gestion des fichiers GCS

### Commandes gcloud utiles
```bash
# Lister la structure compl√®te
gcloud storage ls --recursive gs://supdevinci_bucket/sanda_celia/

# V√©rifier la taille des dossiers
gcloud storage du gs://supdevinci_bucket/sanda_celia/raw/

# T√©l√©charger un √©chantillon
gcloud storage cp gs://supdevinci_bucket/sanda_celia/raw/mouvements-00000-of-00005.csv ./sample.csv

# Supprimer toute la structure (si n√©cessaire)
gcloud storage rm --recursive gs://supdevinci_bucket/sanda_celia/
```

## üîó Int√©gration Databricks

### Configuration de l'acc√®s GCS
1. **Service Account** : Cr√©er un SA avec acc√®s au bucket
2. **Secrets Databricks** : Stocker les cl√©s JSON
3. **Montage GCS** : Configurer l'acc√®s au bucket

### Exemples de code Databricks
```python
# Lecture des donn√©es CSV
df = spark.read.option("header", "true") \
    .csv("gs://supdevinci_bucket/sanda_celia/raw/mouvements-*.csv")

# Sauvegarde optimis√©e en Parquet
df.write.mode("overwrite") \
  .partitionBy("pays", "produit") \
  .parquet("gs://supdevinci_bucket/sanda_celia/staging/mouvements")

# Conversion en Delta Table
df.write.format("delta") \
  .mode("overwrite") \
  .save("gs://supdevinci_bucket/sanda_celia/delta/mouvements")
```

## üìä Analyses possibles

### Exemples d'analyses m√©tier
- **D√©tection d'anomalies** : Montants exceptionnels par type d'op√©ration
- **Analyse des r√©trocessions** : Suivi par agence et pays
- **Tendances temporelles** : √âvolution des cotisations et remboursements
- **Performance des agences** : Volumes et types d'op√©rations
- **Conformit√©** : Suivi des p√©nalit√©s et corrections

## üö® D√©pannage

### Erreurs courantes
```bash
# Erreur d'authentification GCP
gcloud auth application-default login

# Erreur de projet GCP
gcloud config set project YOUR_PROJECT_ID

# Erreur de permissions bucket
gcloud storage buckets get-iam-policy gs://supdevinci_bucket

# R√©installer les d√©pendances
uv sync --reinstall
```

### Variables d'environnement
```bash
# D√©finir le projet GCP
export GCP_PROJECT_ID="biforaplatform"
export GCP_REGION="europe-west1"

# V√©rifier la configuration
uv run python -c "from src.config import Config; print(f'Project: {Config.PROJECT_ID}')"
```

## üìö Ressources

### Documentation
- [Apache Beam Python SDK](https://beam.apache.org/documentation/sdks/python/)
- [Google Cloud Dataflow](https://cloud.google.com/dataflow/docs)
- [uv Documentation](https://docs.astral.sh/uv/)
- [Databricks on GCP](https://docs.databricks.com/en/getting-started/overview.html)

### Liens utiles
- [Faker Documentation](https://faker.readthedocs.io/) - G√©n√©ration de donn√©es synth√©tiques
- [GCS Best Practices](https://cloud.google.com/storage/docs/best-practices)
- [Delta Lake Guide](https://docs.delta.io/latest/index.html)

## üèóÔ∏è Infrastructure as Code (IaC)

### üöÄ D√©ploiement automatis√© avec Terraform

Pour √©viter toutes les configurations manuelles, utilisez le d√©ploiement automatis√© :

```bash
# Installation de Terraform (si pas d√©j√† fait)
# Sur Ubuntu/Debian :
wget -O- https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg
echo "deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main" | sudo tee /etc/apt/sources.list.d/hashicorp.list
sudo apt update && sudo apt install terraform

# Ou avec Homebrew (macOS) :
# brew install terraform

# Authentification GCP (une seule fois)
gcloud auth login
gcloud auth application-default login

# D√©ploiement complet en une commande
./deploy.sh
```

### üìÅ Structure Infrastructure as Code

```
terraform/
‚îú‚îÄ‚îÄ main.tf                    # Configuration principale Terraform
‚îú‚îÄ‚îÄ terraform.tfvars.example  # Variables d'exemple
‚îî‚îÄ‚îÄ terraform.tfvars          # Vos variables (cr√©√© automatiquement)

deploy.sh                     # Script de d√©ploiement automatis√©
```

### üéØ Ce que Terraform d√©ploie automatiquement

1. **Service Account** `dataflow-generator` avec toutes les permissions
2. **R√¥les IAM** pour utilisateur et service account
3. **APIs GCP** (Dataflow, Storage, BigQuery, IAM)
4. **Structure GCS** (tmp/, raw/, staging/, delta/)
5. **Organization Policy** (d√©sactivation contrainte Dataflow)
6. **Lifecycle rules** pour optimisation des co√ªts
7. **Credentials** automatiquement g√©n√©r√©s

### üîß Gestion de l'infrastructure

```bash
# Voir l'√©tat actuel
cd terraform && terraform show

# Planifier des changements
cd terraform && terraform plan

# Appliquer des changements
cd terraform && terraform apply

# D√©truire l'infrastructure
cd terraform && terraform destroy
```

### üí° Avantages de l'approche IaC

- ‚úÖ **Reproductible** : D√©ploiement identique sur diff√©rents environnements
- ‚úÖ **Versionnable** : Infrastructure sous contr√¥le de version Git
- ‚úÖ **Auditable** : Tra√ßabilit√© compl√®te des changements
- ‚úÖ **Scalable** : Facilement adaptable pour d'autres projets
- ‚úÖ **S√©curis√©** : Permissions minimales et bonnes pratiques
- ‚úÖ **Automatis√©** : Z√©ro configuration manuelle

## üìä Monitoring et Observabilit√©

### üîç Surveillance du pipeline

```bash
# Status des jobs Dataflow
gcloud dataflow jobs list --region=europe-west1

# Logs d√©taill√©s d'un job
gcloud dataflow jobs show JOB_ID --region=europe-west1

# M√©triques temps r√©el
gcloud dataflow jobs show JOB_ID --region=europe-west1 --view=JOB_VIEW_ALL
```

### üìà Dashboards recommand√©s

1. **Console Dataflow** : https://console.cloud.google.com/dataflow
2. **Cloud Monitoring** : M√©triques custom et alertes
3. **Cloud Logging** : Recherche et analyse des logs
4. **Cost Management** : Suivi des co√ªts GCS et Dataflow

---

*Projet optimis√© pour uv - Gestionnaire de packages Python ultra-rapide* ‚ö°

*Derni√®re mise √† jour: Janvier 2025*