# üö® Rapport de D√©bogage - Pipeline GCP/Databricks
**Projet :** G√©n√©rateur de Donn√©es Financi√®res Synth√©tiques  
**Date :** 05 Juin 2025  
**Environnement :** Ubuntu 24.04 WSL2, Python 3.12, uv package manager  

---

## üìã R√©sum√© Ex√©cutif

**Objectif :** D√©ployer un pipeline Apache Beam/Dataflow pour g√©n√©rer 10M lignes de donn√©es financi√®res synth√©tiques (~2 Go) et les stocker dans Google Cloud Storage.

**Status Final :** ‚ö†Ô∏è **Pipeline fonctionnel mais bloqu√© par contraintes IAM organisationnelles**

**Temps de r√©solution :** ~2 heures de d√©bogage intensif

---

## üèóÔ∏è Architecture Mise en Place

### Structure Data Lake Cr√©√©e
```
gs://supdevinci_bucket/sanda_celia/
‚îú‚îÄ‚îÄ tmp/              # Fichiers temporaires Dataflow  
‚îú‚îÄ‚îÄ raw/              # Donn√©es brutes CSV (sortie pipeline)
‚îÇ   ‚îî‚îÄ‚îÄ mouvements/   # Donn√©es financi√®res g√©n√©r√©es
‚îú‚îÄ‚îÄ staging/          # Donn√©es optimis√©es Parquet (Databricks)  
‚îÇ   ‚îî‚îÄ‚îÄ mouvements/   # Tables optimis√©es
‚îî‚îÄ‚îÄ delta/            # Tables Delta (Databricks)
    ‚îî‚îÄ‚îÄ mouvements/   # Tables Delta finales
```

### Configuration Technique
- **Projet GCP :** biforaplatform
- **R√©gion :** europe-west1  
- **Bucket :** gs://supdevinci_bucket/sanda_celia
- **Volume cible :** 10M lignes ‚Üí 5 fichiers de ~400MB
- **Secteur :** Financier/Assurance avec 19 types d'op√©rations

---

## üö® Probl√®mes Rencontr√©s et Solutions

### 1. **Erreur de structure des dossiers GCS**
**Probl√®me :** Le script `create_gcs_structure.sh` ne cr√©ait pas les sous-dossiers.

**Erreur :**
```bash
./create_gcs_structure.sh:70: bad substitution
```

**Cause racine :** Syntaxe des tableaux associatifs incompatible entre bash et zsh.

**Solution appliqu√©e :**
```bash
# Remplacement de la logique des tableaux associatifs par des fonctions simples
create_folder() {
    local folder_path=$1
    local description=$2
    echo "Dossier cr√©√© le $(date)" | gcloud storage cp - "$folder_path/.gitkeep"
}

# Cr√©ation manuelle de chaque dossier
create_folder "$BUCKET_BASE/tmp" "tmp (fichiers temporaires)"
create_folder "$BUCKET_BASE/raw/mouvements" "raw/mouvements (donn√©es financi√®res)"
# etc.
```

**R√©sultat :** ‚úÖ Structure cr√©√©e avec succ√®s

---

### 2. **Erreur Application Default Credentials (ADC)**
**Probl√®me :** Apache Beam ne trouvait pas les credentials pour s'authentifier.

**Erreur :**
```
google.auth.exceptions.DefaultCredentialsError: Your default credentials were not found
```

**Solutions tent√©es :**
1. `gcloud auth application-default login` ‚Üí √âchec (probl√®me navigateur WSL2)
2. `gcloud auth application-default login --no-browser` ‚Üí Interruption manuelle

**Solution finale :**
```bash
# Utilisation de la cl√© service account existante
export GOOGLE_APPLICATION_CREDENTIALS="$HOME/key-dataflow.json"
```

**R√©sultat :** ‚úÖ Credentials d√©tect√©s, token d'acc√®s g√©n√©r√©

---

### 3. **Erreur module pip manquant dans uv**
**Probl√®me :** Dataflow ne trouvait pas pip dans l'environnement uv.

**Erreur :**
```
/home/exeio/.venv/bin/python3: No module named pip
Command '[...] -m pip freeze' returned non-zero exit status 1
```

**Solution :**
```bash
uv add pip
```

**R√©sultat :** ‚úÖ Pip ajout√© √† l'environnement, erreur r√©solue

---

### 4. **Erreur permissions IAM Service Account**
**Probl√®me :** L'utilisateur ne peut pas utiliser le service account Dataflow par d√©faut.

**Erreur :**
```
Current user cannot act as service account 115938710183-compute@developer.gserviceaccount.com
Enforced by Org Policy constraint constraints/dataflow.enforceComputeDefaultServiceAccountCheck
```

**Analyses effectu√©es :**
- Service account d√©tect√© : `dataflow-generator@biforaplatform.iam.gserviceaccount.com`
- R√¥les v√©rifi√©s dans IAM policy
- Contraintes organisationnelles identifi√©es

**Solutions tent√©es :**

#### A. Ajout de permissions utilisateur
```bash
gcloud projects add-iam-policy-binding biforaplatform \
  --member="user:andrirazafy9@gmail.com" \
  --role="roles/iam.serviceAccountUser"
```
**R√©sultat :** ‚úÖ R√¥le ajout√© mais contraintes org persistent

#### B. Configuration service account explicite
```python
gcloud_opts.service_account_email = "dataflow-generator@biforaplatform.iam.gserviceaccount.com"
```
**R√©sultat :** ‚ùå M√™me erreur de permissions

#### C. Utilisation credentials utilisateur
```python
# Suppression de la sp√©cification du service account
# gcloud_opts.service_account_email = "..."
```
**R√©sultat :** ‚ùå Contrainte `dataflow.enforceComputeDefaultServiceAccountCheck` active

---

## üîç Analyse des Contraintes Organisationnelles

### Politiques IAM D√©tect√©es
D'apr√®s `gcloud projects get-iam-policy biforaplatform` :

1. **Contraintes temporelles :** Plusieurs r√¥les avec conditions d'expiration
2. **Contraintes Databricks :** Ressources prot√©g√©es par conditions complexes  
3. **Politique org :** `constraints/dataflow.enforceComputeDefaultServiceAccountCheck`

### Configuration IAM Actuelle
```yaml
# R√¥les utilisateur confirm√©s
- user:andrirazafy9@gmail.com ‚Üí roles/owner
- user:andrirazafy9@gmail.com ‚Üí roles/iam.serviceAccountUser

# Service accounts configur√©s  
- dataflow-generator@biforaplatform.iam.gserviceaccount.com ‚Üí roles/dataflow.admin
- dataflow-generator@biforaplatform.iam.gserviceaccount.com ‚Üí roles/dataflow.worker
- dataflow-generator@biforaplatform.iam.gserviceaccount.com ‚Üí roles/storage.admin
```

---

## üö¶ √âtat Final du Pipeline

### ‚úÖ √âl√©ments Fonctionnels
1. **Authentification :** Service account key fonctionnelle
2. **Structure GCS :** Dossiers cr√©√©s et accessibles
3. **Code Pipeline :** Logique m√©tier valid√©e (19 types d'op√©rations)
4. **Environnement uv :** D√©pendances install√©es correctement
5. **Upload Staging :** Fichiers pipeline upload√©s avec succ√®s

### ‚ùå Blocage Restant
**Contrainte organisationnelle :** `constraints/dataflow.enforceComputeDefaultServiceAccountCheck`

Cette contrainte emp√™che l'utilisation de service accounts par d√©faut et n√©cessite une intervention administrateur organisation GCP.

---

## üéØ Solutions Recommand√©es

### Solution 1 : R√©solution Administrative (Recommand√©e)
**Action :** Contacter l'administrateur GCP pour :
```bash
# D√©sactiver la contrainte organisationnelle
gcloud org-policies delete constraints/dataflow.enforceComputeDefaultServiceAccountCheck \
  --organization=ORGANIZATION_ID
```

### Solution 2 : Mode Local de D√©veloppement
**Action :** Utiliser DirectRunner pour tests :
```python
options.view_as(StandardOptions).runner = "DirectRunner"
```
**Limite :** Performance r√©duite, pas de parall√©lisation cloud

### Solution 3 : Migration vers un projet sans contraintes
**Action :** Cr√©er un nouveau projet GCP d√©di√© sans contraintes organisationnelles.

---

## üìä Donn√©es de Test G√©n√©r√©es

### Schema CSV Final
| Champ          | Type   | Description                 | Exemple                         |
| -------------- | ------ | --------------------------- | ------------------------------- |
| `id_mouvement` | string | Identifiant unique          | M0000001                        |
| `date_op`      | date   | Date de l'op√©ration         | 2025-06-15                      |
| `produit`      | string | Type produit d'assurance    | auto, sant√©, habitation, vie    |
| `type_op`      | string | Type d'op√©ration financi√®re | cotisation, remboursement, etc. |
| `montant`      | float  | Montant en euros            | 1250.50                         |
| `agence_id`    | string | Identifiant agence          | AG_001                          |
| `pays`         | string | Code pays europ√©en          | FR, ES, IT, DE, BE              |

### Types d'Op√©rations Impl√©ment√©s (19 types)
- **Base :** cotisation, remboursement, commission
- **Ajustements :** r√©trocession, ajustement_cotisation, r√©gularisation  
- **P√©nalit√©s :** p√©nalit√©_retard, correction_comptable, annulation
- **Internes :** virement_interne, provision_sinistre, frais_gestion
- **Autres :** prime_exceptionnelle, bonus_malus, etc.

### Logique M√©tier des Montants
```python
# Exemples de distribution r√©aliste
cotisation: 50‚Ç¨ ‚Üí 2500‚Ç¨ (positif)
remboursement: -100‚Ç¨ ‚Üí -5000‚Ç¨ (n√©gatif)  
p√©nalit√©s: 15‚Ç¨ ‚Üí 300‚Ç¨ (positif)
ajustements: -200‚Ç¨ ‚Üí +200‚Ç¨ (bi-directionnel)
```

---

## üîß Configuration Technique Valid√©e

### Environnement uv
```toml
[project]
name = "gcp-databricks-project"
version = "0.1.0"
requires-python = ">=3.12"
dependencies = [
    "apache-beam[gcp]>=2.65.0",
    "faker>=37.3.0", 
    "pip>=25.1.1"
]
```

### Variables d'Environnement
```bash
export GOOGLE_APPLICATION_CREDENTIALS="$HOME/key-dataflow.json"
export GCP_PROJECT_ID="biforaplatform"  
export GCP_REGION="europe-west1"
```

### Commandes de Validation
```bash
# Test credentials
gcloud auth application-default print-access-token

# Test acc√®s bucket  
gcloud storage ls gs://supdevinci_bucket/sanda_celia/

# Validation configuration
uv run python -c "from src.config import Config; Config.validate_config()"
```

---

## üìà M√©triques de Performance

### Uploads R√©ussis
```
‚úÖ submission_environment_dependencies.txt (0 secondes)
‚úÖ pipeline.pb (0-1 secondes)  
‚úÖ Fichiers staging upload√©s vers GCS
```

### Warnings Non-Critiques
```
‚ö†Ô∏è Bucket soft-delete policy enabled (facturation optimisable)
‚ö†Ô∏è Additional dependencies in SDK container (performance optimisable)
```

---

## üöÄ Prochaines √âtapes

### Imm√©diat (Post-r√©solution contraintes)
1. **Lancer pipeline :** `uv run src/generate_to_gcs.py`
2. **Monitoring :** `gcloud dataflow jobs list --region=europe-west1`
3. **Validation :** `uv run src/verify_data.py --sample`

### Court terme
1. **Int√©gration Databricks :** Conversion CSV ‚Üí Parquet ‚Üí Delta
2. **Optimisation :** Partitioning par pays/produit  
3. **Monitoring :** Alertes et dashboards

### Long terme  
1. **Automatisation :** Cloud Scheduler pour g√©n√©ration mensuelle
2. **Pipeline ML :** D√©tection d'anomalies sur les donn√©es
3. **API :** Exposition des donn√©es via Cloud Functions

---

## üìö Documentation Cr√©√©e

### Fichiers de Configuration
- ‚úÖ `src/config.py` - Configuration centralis√©e
- ‚úÖ `src/generate_to_gcs.py` - Pipeline principal  
- ‚úÖ `src/verify_data.py` - Utilitaires de validation
- ‚úÖ `pyproject.toml` - D√©pendances uv
- ‚úÖ `README.md` - Documentation utilisateur (mise √† jour uv)
- ‚úÖ `create_gcs_structure.sh` - Script d'initialisation
- ‚úÖ `GCP_GCLOUD_CHEATSHEET.md` - Commandes de r√©f√©rence

### Scripts Utilitaires
```bash
# Cr√©ation structure
./create_gcs_structure.sh

# V√©rification donn√©es  
uv run src/verify_data.py --stats

# Guide Databricks
uv run src/generate_to_gcs.py --guide
```

---

## üéì Enseignements Tir√©s

### Bonnes Pratiques
1. **uv vs pip :** uv offre des performances sup√©rieures pour la gestion d'environnements
2. **Service Accounts :** Pr√©f√©rer les SAs d√©di√©s aux credentials utilisateur
3. **Structure GCS :** Organisation claire Raw/Staging/Delta facilite l'int√©gration
4. **Monitoring :** Logs d√©taill√©s essentiels pour le d√©bogage

### Pi√®ges √âvit√©s  
1. **Syntaxe Shell :** Diff√©rences bash/zsh dans les scripts
2. **ADC vs SA Keys :** Confusion entre types d'authentification
3. **Contraintes Org :** Impact sur les permissions individuelles
4. **Environnements Python :** Sp√©cificit√©s uv vs virtualenv classique

### Am√©liorations Futures
1. **Tests Unitaires :** Validation logique m√©tier
2. **CI/CD :** Automatisation d√©ploiement  
3. **Infrastructure as Code :** Terraform pour ressources GCP
4. **Observabilit√© :** M√©triques custom et alerting

---

## üìû Support et Escalade

### Contacts Techniques
- **GCP Admin :** R√©solution contraintes organisationnelles
- **Databricks Team :** Configuration int√©gration GCS
- **√âquipe Projet :** andrirazafy9@gmail.com

### Ressources de R√©f√©rence
- [Documentation Apache Beam](https://beam.apache.org/documentation/)
- [GCP IAM Troubleshooting](https://cloud.google.com/iam/docs/troubleshooting)
- [uv Documentation](https://docs.astral.sh/uv/)
- [Databricks GCS Integration](https://docs.databricks.com/external-data/gcs.html)

---

**Fin du Rapport - Pr√©par√© avec ‚ù§Ô∏è et beaucoup de caf√© ‚òï**

*Ce rapport documumente 2h de d√©bogage intensif et constitue une base de connaissance pour les futurs projets similaires.* 